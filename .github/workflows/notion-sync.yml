name: Sync Supabase → Notion

on:
  schedule:
    - cron: "0 15 * * *"   # 11:00 in New York during EDT (UTC-4)
    - cron: "0 16 * * *"   # 11:00 in New York during EST (UTC-5)
  workflow_dispatch:
    inputs:
      chunk_offset:
        description: "Start offset"
        required: false
        default: "0"
      max_rows:
        description: "Max rows per run"
        required: false
        default: "800"
      max_minutes:
        description: "Max minutes per run"
        required: false
        default: "110"
      chunk_size:
        description: "Rows per page (SQL LIMIT)"
        required: false
        default: "600"
      rate_delay:
        description: "Seconds between writes"
        required: false
        default: "0.35"
      username_filter:
        description: "Optional username filter (e.g., HyMatrixOrg)"
        required: false
        default: ""
      import_all:
        description: "Ignore lookback window and fetch ALL history (true/false)"
        required: false
        default: "false"

permissions:
  actions: write
  contents: read

concurrency:
  group: notion-sync-${{ github.ref }}
  cancel-in-progress: true

jobs:
  sync:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: "4.3.3"
          use-public-rspm: true

      - name: Install system libraries (for RPostgres/curl)
        run: |
          sudo apt-get update
          sudo apt-get install -y libpq-dev libcurl4-openssl-dev libssl-dev

      - name: Ensure GitHub CLI is installed
        run: |
          if ! command -v gh >/dev/null 2>&1; then
            sudo apt-get update
            sudo apt-get install -y gh
          fi

      - id: nytime
        name: Get New York hour
        run: echo "hour=$(TZ=America/New_York date +%H)" >> "$GITHUB_OUTPUT"

      # ✅ Cache common R packages to save setup time
      - name: Cache R deps
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: |
            any::httr2
            any::jsonlite
            any::DBI
            any::RPostgres
          cache-version: 1

      # ✅ Preflight check with retry logic (handles 429 rate limits)
      - name: Preflight – Notion auth check (with retries)
        env:
          NOTION_TOKEN:       ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        run: |
          set -e
          url="https://api.notion.com/v1/databases/${NOTION_DATABASE_ID}"
          tries=0
          max_tries=8
          while :; do
            tries=$((tries+1))
            code=$(curl -s -D /tmp/headers -o /tmp/notion.json -w "%{http_code}" \
              -H "Authorization: Bearer ${NOTION_TOKEN}" \
              -H "Notion-Version: 2022-06-28" \
              "$url")
            echo "HTTP ${code}"

            if [ "$code" = "200" ]; then
              echo "✅ Notion DB schema fetch OK."
              break
            fi

            if [ "$code" = "429" ] && [ $tries -lt $max_tries ]; then
              ra=$(awk 'tolower($1)=="retry-after:"{print $2}' /tmp/headers | tr -d '\r')
              if [ -n "$ra" ]; then
                echo "Rate limited. Retry-After: ${ra}s"
                sleep "$ra"
              else
                # exponential backoff with jitter, capped at 10s
                wait=$(python3 - <<PY
import math, random
t = $tries
print(min(10, 0.5*(2**(t-1)) + random.random()))
PY
)
                echo "Rate limited. Backing off ${wait}s"
                sleep "$wait"
              fi
              continue
            fi

            echo "::error title=Notion API failed::See payload below (fields redacted)."
            sed -E 's/"access_token":"[^"]+"/"access_token":"***"/g' /tmp/notion.json | head -200
            exit 1
          done

      - id: run
        name: Run sync
        if: ${{ github.event_name != 'schedule' || steps.nytime.outputs.hour == '11' }}
        env:
          NOTION_TOKEN:        ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID:  ${{ secrets.NOTION_DATABASE_ID }}
          SUPABASE_HOST:       ${{ secrets.SUPABASE_HOST }}
          SUPABASE_USER:       ${{ secrets.SUPABASE_USER }}
          SUPABASE_PWD:        ${{ secrets.SUPABASE_PWD }}
          SUPABASE_DB:         ${{ secrets.SUPABASE_DB }}
          SUPABASE_PORT:       ${{ secrets.SUPABASE_PORT }}
          LOOKBACK_HOURS:      "17532"
          # Resumability knobs
          CHUNK_OFFSET:        ${{ inputs.chunk_offset   || '0' }}
          MAX_ROWS_PER_RUN:    ${{ inputs.max_rows       || '800' }}
          MAX_MINUTES:         ${{ inputs.max_minutes    || '110' }}
          # Performance knobs
          CHUNK_SIZE:          ${{ inputs.chunk_size     || '600' }}
          RATE_DELAY_SEC:      ${{ inputs.rate_delay     || '0.35' }}
          # Filters
          USERNAME_FILTER:     ${{ inputs.username_filter || '' }}
          IMPORT_ALL:          ${{ inputs.import_all      || 'false' }}
        run: Rscript sync_notion.R

      - name: Auto-continue next chunk
        if: ${{ steps.run.outputs.next_offset && steps.run.outputs.next_offset != 'done' }}
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "Queuing next run from offset ${{ steps.run.outputs.next_offset }}..."
          gh workflow run "${{ github.workflow }}" \
            -f chunk_offset=${{ steps.run.outputs.next_offset }} \
            -f max_rows=${{ inputs.max_rows || '800' }} \
            -f max_minutes=${{ inputs.max_minutes || '110' }} \
            -f chunk_size=${{ inputs.chunk_size || '600' }} \
            -f rate_delay=${{ inputs.rate_delay || '0.35' }} \
            -f username_filter="${{ inputs.username_filter || '' }}" \
            -f import_all=${{ inputs.import_all || 'false' }}

